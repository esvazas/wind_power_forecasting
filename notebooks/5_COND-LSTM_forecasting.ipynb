{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "consistent-hospital",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cond_rnn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from solver import processing\n",
    "from solver import lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elder-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FEATURE PARAMETERS\n",
    "# prediction target\n",
    "feature_predict = 'P_avg'\n",
    "# prediction inputs from turbine data\n",
    "features_train = ['P_avg', 'Ba_avg', 'Wa_avg', 'Wx', 'Wy']\n",
    "# engineered prediction inputs \n",
    "features_cond = ['Day sin', 'Day cos', 'Year sin', 'Year cos']\n",
    "\n",
    "### TRAIN/VAL/TEST SPLIT\n",
    "train_years = [2013, 2014, 2015, 2016]\n",
    "validation_years = [2017]\n",
    "test_years = [2017]\n",
    "\n",
    "### TRAINING PARAMETERS:\n",
    "BATCH_SIZE = 512\n",
    "N_EPOCHS = 100\n",
    "N_NEURONS = 64\n",
    "\n",
    "### FEATURE ENGINEERING PARAMETERS\n",
    "MA_CONSTANT = 3 # moving average smoothing parameter\n",
    "N_OUT = 12 # forecast horizon\n",
    "N_PAST = 48 # number of autoregression samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "packed-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets\n",
    "TURBINE_ID = 'R80711'\n",
    "DATA_DIR = os.path.join('../datasets/after_imputation', 'turbine_{}.csv'.format(TURBINE_ID))\n",
    "\n",
    "# read datasets\n",
    "dataset = processing.read_dataset(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8aeb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define masks for training/validation and testing (will be used later)\n",
    "train_idx = dataset[dataset['Date_time'].dt.year.isin(train_years)].index\n",
    "valid_idx = dataset[dataset['Date_time'].dt.year.isin(validation_years)].index\n",
    "test_idx = dataset[dataset['Date_time'].dt.year.isin(test_years)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b0882e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: \t\t 0\n",
      "Number of rows with nan: \t 0\n"
     ]
    }
   ],
   "source": [
    "# some stats:\n",
    "print(\"Number of duplicates: \\t\\t {}\".format(len(dataset.index[dataset.index.duplicated()].unique())))\n",
    "print(\"Number of rows with nan: \\t {}\".format(np.count_nonzero(dataset.isnull())))\n",
    "\n",
    "# perform smoothing\n",
    "if feature_predict in features_train:\n",
    "    dataset = processing.smooth(dataset, cols_to_smooth=features_train, ma_constant=MA_CONSTANT)\n",
    "else:\n",
    "    dataset = processing.smooth(dataset, cols_to_smooth=features_train+[feature_predict], ma_constant=MA_CONSTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4facc784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dates for plotting\n",
    "test_dates = dataset.loc[dataset['Date_time'].dt.year.isin(test_years), 'Date_time'].values\n",
    "\n",
    "# split to training/validation/testing sets based on indices\n",
    "dataset_train = dataset[dataset.index.isin(train_idx)].copy()\n",
    "dataset_valid = dataset[dataset.index.isin(valid_idx)].copy()\n",
    "dataset_test = dataset[dataset.index.isin(test_idx)].copy()\n",
    "\n",
    "# define target mask for features\n",
    "target_idx = np.where(dataset_train.columns == feature_predict)[0][0]\n",
    "target_mask = np.zeros((dataset_train.shape[1])).astype(bool)\n",
    "target_mask[target_idx] = True\n",
    "# define input mask for autoregression features\n",
    "input_idx = [np.where(dataset_train.columns == feat_col)[0][0] for feat_col in features_train+features_cond]\n",
    "input_mask = np.zeros((dataset_train.shape[1])).astype(bool)\n",
    "input_mask[input_idx] = True\n",
    "\n",
    "# apply masks\n",
    "y_train = dataset_train.iloc[:, target_mask]\n",
    "y_valid = dataset_valid.iloc[:, target_mask]\n",
    "y_test = dataset_test.iloc[:, target_mask]\n",
    "X_train = dataset_train.iloc[:, input_mask]\n",
    "X_valid = dataset_valid.iloc[:, input_mask]\n",
    "X_test = dataset_test.iloc[:, input_mask]\n",
    "\n",
    "# define mask for conditional features\n",
    "cond_idx = [np.where(X_train.columns == feat_col)[0][0] for feat_col in features_cond]\n",
    "cond_mask = np.zeros((X_train.shape[1])).astype(bool)\n",
    "cond_mask[cond_idx] = True\n",
    "\n",
    "# Define scaler and fit only on training data\n",
    "scaler_output = MinMaxScaler()\n",
    "y_train = scaler_output.fit_transform(y_train)\n",
    "y_valid = scaler_output.transform(y_valid)\n",
    "y_test = scaler_output.transform(y_test)\n",
    "# Define scaler and fit only on training data\n",
    "scaler_input = MinMaxScaler()\n",
    "X_train = scaler_input.fit_transform(X_train)\n",
    "X_valid = scaler_input.transform(X_valid)\n",
    "X_test = scaler_input.transform(X_test)\n",
    "\n",
    "# Make small tests\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_valid.shape[0] == y_valid.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59177fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X variables to autoregression and conditional variables\n",
    "X_train_ar = X_train[:, ~cond_mask]\n",
    "X_valid_ar = X_valid[:, ~cond_mask]\n",
    "X_test_ar = X_test[:, ~cond_mask]\n",
    "X_train_cond = X_train[:, cond_mask]\n",
    "X_valid_cond = X_valid[:, cond_mask]\n",
    "X_test_cond = X_test[:, cond_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afda4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make supervised learning problem\n",
    "X_train_ar = processing.series_to_supervised(X_train_ar, n_in=N_PAST, n_out=0, dropnan=True)\n",
    "X_train_cond = processing.series_to_supervised(X_train_cond, n_in=1, n_out=0, dropnan=True)\n",
    "y_train = processing.series_to_supervised(y_train, n_in=N_PAST, n_out=N_OUT, dropnan=True).iloc[:,-1]\n",
    "X_valid_ar = processing.series_to_supervised(X_valid_ar, n_in=N_PAST, n_out=0, dropnan=True)\n",
    "X_valid_cond = processing.series_to_supervised(X_valid_cond, n_in=1, n_out=0, dropnan=True)\n",
    "y_valid = processing.series_to_supervised(y_valid, n_in=N_PAST, n_out=N_OUT, dropnan=True).iloc[:,-1]\n",
    "X_test_ar = processing.series_to_supervised(X_test_ar, n_in=N_PAST, n_out=0, dropnan=True)\n",
    "X_test_cond = processing.series_to_supervised(X_test_cond, n_in=1, n_out=0, dropnan=True)\n",
    "y_test = processing.series_to_supervised(y_test, n_in=N_PAST, n_out=N_OUT, dropnan=True).iloc[:,-1]\n",
    "\n",
    "# Align X with y\n",
    "X_train_ar = X_train_ar[X_train_ar.index.isin(y_train.index)]\n",
    "X_valid_ar = X_valid_ar[X_valid_ar.index.isin(y_valid.index)]\n",
    "X_test_ar = X_test_ar[X_test_ar.index.isin(y_test.index)]\n",
    "X_train_cond = X_train_cond[X_train_cond.index.isin(y_train.index)]\n",
    "X_valid_cond = X_valid_cond[X_valid_cond.index.isin(y_valid.index)]\n",
    "X_test_cond = X_test_cond[X_test_cond.index.isin(y_test.index)]\n",
    "\n",
    "# Set to numpy arrays\n",
    "X_train = X_train_ar.values\n",
    "X_train_cond = X_train_cond.values\n",
    "y_train = y_train.values\n",
    "X_valid = X_valid_ar.values\n",
    "X_valid_cond = X_valid_cond.values\n",
    "y_valid = y_valid.values\n",
    "X_test = X_test_ar.values\n",
    "X_test_cond = X_test_cond.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f92614ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cond_rnn import ConditionalRNN\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def fit_cond_lstm(X, y, X_val, y_val, X_extra, X_val_extra, batch_size, nb_epochs, neurons, n_past):\n",
    "    X = X.reshape(-1, n_past, int(X.shape[1] / n_past))\n",
    "    X_val = X_val.reshape(-1, n_past, int(X_val.shape[1] / n_past))\n",
    "    \n",
    "    print(X.shape, X_extra.shape)\n",
    "\n",
    "    # define model\n",
    "    model = Sequential(layers=[\n",
    "        ConditionalRNN(neurons, cell='LSTM'),\n",
    "        Dense(units=1, activation='linear') # regression problem.\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit model\n",
    "    history = model.fit(\n",
    "        [X, X_extra], y, epochs=nb_epochs, batch_size=batch_size,\n",
    "        verbose=1, validation_data=([X_val, X_val_extra], y_val),\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='val_loss', patience=10, mode='auto')\n",
    "        ])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edf1a69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35003, 48, 5) (35003, 4)\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'tuple'> input: (<tf.Tensor 'IteratorGetNext:0' shape=(None, 48, 5) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 4) dtype=float32>)\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'tuple'> input: (<tf.Tensor 'IteratorGetNext:0' shape=(None, 48, 5) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 4) dtype=float32>)\n",
      "Consider rewriting this model with the Functional API.\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.0446WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'tuple'> input: (<tf.Tensor 'IteratorGetNext:0' shape=(None, 48, 5) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 4) dtype=float32>)\n",
      "Consider rewriting this model with the Functional API.\n",
      "69/69 [==============================] - 18s 231ms/step - loss: 0.0444 - val_loss: 0.0334\n",
      "Epoch 2/100\n",
      "69/69 [==============================] - 14s 208ms/step - loss: 0.0305 - val_loss: 0.0328\n",
      "Epoch 3/100\n",
      "69/69 [==============================] - 14s 205ms/step - loss: 0.0303 - val_loss: 0.0322\n",
      "Epoch 4/100\n",
      "69/69 [==============================] - 14s 200ms/step - loss: 0.0288 - val_loss: 0.0311\n",
      "Epoch 5/100\n",
      "69/69 [==============================] - 14s 204ms/step - loss: 0.0294 - val_loss: 0.0309\n",
      "Epoch 6/100\n",
      "69/69 [==============================] - 16s 232ms/step - loss: 0.0282 - val_loss: 0.0305\n",
      "Epoch 7/100\n",
      "69/69 [==============================] - 16s 237ms/step - loss: 0.0277 - val_loss: 0.0311\n",
      "Epoch 8/100\n",
      "69/69 [==============================] - 22s 321ms/step - loss: 0.0274 - val_loss: 0.0309\n",
      "Epoch 9/100\n",
      "69/69 [==============================] - 24s 348ms/step - loss: 0.0271 - val_loss: 0.0301\n",
      "Epoch 10/100\n",
      "69/69 [==============================] - 25s 358ms/step - loss: 0.0272 - val_loss: 0.0304\n",
      "Epoch 11/100\n",
      "69/69 [==============================] - 21s 300ms/step - loss: 0.0273 - val_loss: 0.0301\n",
      "Epoch 12/100\n",
      "69/69 [==============================] - 21s 298ms/step - loss: 0.0271 - val_loss: 0.0300\n",
      "Epoch 13/100\n",
      "69/69 [==============================] - 20s 294ms/step - loss: 0.0263 - val_loss: 0.0295\n",
      "Epoch 14/100\n",
      "69/69 [==============================] - 20s 298ms/step - loss: 0.0271 - val_loss: 0.0300\n",
      "Epoch 15/100\n",
      "69/69 [==============================] - 20s 296ms/step - loss: 0.0268 - val_loss: 0.0299\n",
      "Epoch 16/100\n",
      "69/69 [==============================] - 24s 347ms/step - loss: 0.0265 - val_loss: 0.0298\n",
      "Epoch 17/100\n",
      "69/69 [==============================] - 21s 303ms/step - loss: 0.0260 - val_loss: 0.0305\n",
      "Epoch 18/100\n",
      "69/69 [==============================] - 23s 336ms/step - loss: 0.0262 - val_loss: 0.0297\n",
      "Epoch 19/100\n",
      "69/69 [==============================] - 23s 335ms/step - loss: 0.0258 - val_loss: 0.0299\n",
      "Epoch 20/100\n",
      "69/69 [==============================] - 21s 303ms/step - loss: 0.0250 - val_loss: 0.0309\n",
      "Epoch 21/100\n",
      "69/69 [==============================] - 19s 275ms/step - loss: 0.0250 - val_loss: 0.0303\n",
      "Epoch 22/100\n",
      "69/69 [==============================] - 20s 293ms/step - loss: 0.0246 - val_loss: 0.0315\n",
      "Epoch 23/100\n",
      "69/69 [==============================] - 21s 308ms/step - loss: 0.0255 - val_loss: 0.0317\n"
     ]
    }
   ],
   "source": [
    "lstm_model, history = fit_cond_lstm(\n",
    "    X_train, y_train, \n",
    "    X_valid, y_valid, \n",
    "    X_train_cond, X_valid_cond,\n",
    "    BATCH_SIZE, N_EPOCHS, N_NEURONS, \n",
    "    n_past=N_PAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3352a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, X, X_extra, y, scaler, n_past):\n",
    "    # shape input data\n",
    "    X = X.reshape(-1, n_past, int(X.shape[1]/n_past))\n",
    "    # make predictions\n",
    "    y_predict = model.predict([X, X_extra])\n",
    "    # scale outputs back\n",
    "    y_true = scaler.inverse_transform(y.reshape(-1,1)).flatten()\n",
    "    y_predict = scaler.inverse_transform(y_predict.reshape(-1,1)).flatten()\n",
    "    # evaluate predictions\n",
    "    print(\"MAE: {} RMSE: {}\".format(\n",
    "        round(mean_absolute_error(y_true, y_predict), 3),\n",
    "        round(np.sqrt(mean_squared_error(y_true, y_predict)),3)))\n",
    "    return y_predict, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e22f1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'tuple'> input: (<tf.Tensor 'IteratorGetNext:0' shape=(None, 48, 5) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 4) dtype=float32>)\n",
      "Consider rewriting this model with the Functional API.\n",
      "MAE: 264.505 RMSE: 365.795\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "y_predict, y_true = make_predictions(lstm_model, X_valid, X_valid_cond, y_valid, scaler_output, N_PAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d4fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With time features (LSTM) Horizon  1: MAE: 34.281 RMSE: 52.417\n",
    "# With time features (LSTM) Horizon 12: MAE: 262.161 RMSE: 355.49\n",
    "# With all features:  Horizon 12: MAE: 264.505 RMSE: 365.795\n",
    "\n",
    "# Without time features horizon 1: MAE: 35.562 RMSE: 54.093 sMAPE: 0.361\n",
    "# Without time features Horizon 12: MAE: 261.492 RMSE: 358.674 sMAPE: 0.877\n",
    "\n",
    "# ---- (Lets try univariate + info):\n",
    "# Horizon 1: MAE: 35.653 RMSE: 54.978 sMAPE: 0.394\n",
    "# Horizon 12: MAE: 271.414 RMSE: 368.373"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
