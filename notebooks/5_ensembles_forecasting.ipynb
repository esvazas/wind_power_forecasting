{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from solver import processing\n",
    "from solver import ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FEATURE PARAMETERS\n",
    "# prediction target\n",
    "feature_predict = 'P_avg'\n",
    "# prediction inputs from turbine data\n",
    "features_train = ['P_avg']\n",
    "# engineered prediction inputs \n",
    "engineered_features = []\n",
    "\n",
    "### TRAIN/VAL/TEST SPLIT\n",
    "train_years = [2013, 2014, 2015]\n",
    "validation_years = [2016]\n",
    "test_years = [2017]\n",
    "\n",
    "### ENSEMBLE PARAMETERS:\n",
    "N_DT = 100  # (int) number of decision trees\n",
    "N_SVR = 64  # (int) number of svr\n",
    "S_DT = 500  # (int) number of samples for training each decision tree\n",
    "S_SVR = 500 # (int) number of samples for training each svr\n",
    "\n",
    "### FEATURE ENGINEERING PARAMETERS\n",
    "MA_CONSTANT = 3 # moving average smoothing parameter\n",
    "N_OUT = 1 # forecast horizon\n",
    "N_PAST = 12 # number of autoregression samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets\n",
    "TURBINE_ID = 'R80711'\n",
    "DATA_DIR = os.path.join('../datasets/after_imputation', 'turbine_{}.csv'.format(TURBINE_ID))\n",
    "\n",
    "# read datasets\n",
    "dataset = processing.read_dataset(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e34e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define masks for training/validation and testing (will be used later)\n",
    "train_idx = dataset[dataset['Date_time'].dt.year.isin(train_years)].index\n",
    "valid_idx = dataset[dataset['Date_time'].dt.year.isin(validation_years)].index\n",
    "test_idx = dataset[dataset['Date_time'].dt.year.isin(test_years)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some stats:\n",
    "print(\"Number of duplicates: \\t\\t {}\".format(len(dataset.index[dataset.index.duplicated()].unique())))\n",
    "print(\"Number of rows with nan: \\t {}\".format(np.count_nonzero(dataset.isnull())))\n",
    "\n",
    "# perform smoothing\n",
    "if feature_predict in features_train:\n",
    "    dataset = processing.smooth(dataset, cols_to_smooth=features_train, ma_constant=MA_CONSTANT)\n",
    "else:\n",
    "    dataset = processing.smooth(dataset, cols_to_smooth=features_train+[feature_predict], ma_constant=MA_CONSTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dates for plotting\n",
    "test_dates = dataset.loc[dataset['Date_time'].dt.year.isin(test_years), 'Date_time'].values\n",
    "\n",
    "# split to training/validation/testing sets based on indices\n",
    "dataset_train = dataset[dataset.index.isin(train_idx)].copy()\n",
    "dataset_valid = dataset[dataset.index.isin(valid_idx)].copy()\n",
    "dataset_test = dataset[dataset.index.isin(test_idx)].copy()\n",
    "\n",
    "# define target mask for features\n",
    "target_idx = np.where(dataset_train.columns == feature_predict)[0][0]\n",
    "target_mask = np.zeros((dataset_train.shape[1])).astype(bool)\n",
    "target_mask[target_idx] = True\n",
    "# define input mask for features\n",
    "input_idx = [np.where(dataset_train.columns == feat_col)[0][0] for feat_col in features_train+engineered_features]\n",
    "input_mask = np.zeros((dataset_train.shape[1])).astype(bool)\n",
    "input_mask[input_idx] = True\n",
    "\n",
    "# Define scaler and fit only on training data\n",
    "scaler_output = MinMaxScaler()\n",
    "y_train = scaler_output.fit_transform(dataset_train.iloc[:, target_mask])\n",
    "y_valid = scaler_output.transform(dataset_valid.iloc[:, target_mask])\n",
    "y_test = scaler_output.transform(dataset_test.iloc[:, target_mask])\n",
    "# Define scaler and fit only on training data\n",
    "scaler_input = MinMaxScaler()\n",
    "X_train = scaler_input.fit_transform(dataset_train.iloc[:, input_mask])\n",
    "X_valid = scaler_input.transform(dataset_valid.iloc[:, input_mask])\n",
    "X_test = scaler_input.transform(dataset_test.iloc[:, input_mask])\n",
    "\n",
    "# Make small tests\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_valid.shape[0] == y_valid.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532bc600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make supervised learning problem\n",
    "X_train_sup = processing.series_to_supervised(X_train, n_in=N_PAST, n_out=0, dropnan=True)\n",
    "y_train_sup = processing.series_to_supervised(y_train, n_in=N_PAST, n_out=N_OUT, dropnan=True).iloc[:,-1]\n",
    "X_valid_sup = processing.series_to_supervised(X_valid, n_in=N_PAST, n_out=0, dropnan=True)\n",
    "y_valid_sup = processing.series_to_supervised(y_valid, n_in=N_PAST, n_out=N_OUT, dropnan=True).iloc[:,-1]\n",
    "X_test_sup = processing.series_to_supervised(X_test, n_in=N_PAST, n_out=0, dropnan=True)\n",
    "y_test_sup = processing.series_to_supervised(y_test, n_in=N_PAST, n_out=N_OUT, dropnan=True).iloc[:,-1]\n",
    "\n",
    "# Align X with y\n",
    "X_train_sup = X_train_sup[X_train_sup.index.isin(y_train_sup.index)]\n",
    "X_valid_sup = X_valid_sup[X_valid_sup.index.isin(y_valid_sup.index)]\n",
    "X_test_sup = X_test_sup[X_test_sup.index.isin(y_test_sup.index)]\n",
    "\n",
    "# Set to numpy arrays\n",
    "X_train = X_train_sup.values\n",
    "y_train = y_train_sup.values\n",
    "X_valid = X_valid_sup.values\n",
    "y_valid = y_valid_sup.values\n",
    "X_test = X_test_sup.values\n",
    "y_test = y_test_sup.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train FIRST ensemble\n",
    "base_reg = DecisionTreeRegressor(min_samples_split=2, max_depth=50)\n",
    "reg_estimators1, weights1 = ensembles.train_homogeneous_ensemble(base_reg, X_train, y_train, S_DT, N_DT)\n",
    "predictions = ensembles.make_predictions(reg_estimators1, weights1, scaler_output, X_test, y_test, N_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f2399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAE: 50.401 RMSE: 77.782 sMAPE: 0.392\n",
    "#MAE: 36.034 RMSE: 54.458 sMAPE: 0.363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SECOND ensemble\n",
    "base_reg = SVR(C=10000, epsilon=0.01, gamma=0.001, kernel='rbf')\n",
    "reg_estimators2, weights2 = ensembles.train_homogeneous_ensemble(base_reg, X_train, y_train, S_SVR, N_SVR)\n",
    "predictions = ensembles.make_predictions(reg_estimators2, weights2, scaler_output, X_test, y_test, N_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8764979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train COMBINED ensemble\n",
    "base_reg1 = DecisionTreeRegressor(min_samples_split=2, max_depth=50)\n",
    "base_reg2 = SVR(C=10000, epsilon=0.01, gamma=0.001, kernel='rbf')\n",
    "predictions = ensembles.train_and_predict_heterogeneous_ensemble(\n",
    "    X_train, y_train, X_test, y_test, [S_DT, S_SVR], [N_DT, N_SVR], N_OUT, base_reg1, base_reg2, scaler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9603627",
   "metadata": {},
   "source": [
    "# Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale true values back\n",
    "true = scaler_output.inverse_transform(y_test.reshape(-1,1)).flatten()\n",
    "# plot predictions\n",
    "if N_OUT > 1:\n",
    "    predictions = pd.Series(predictions, index=test_dates[N_PAST:-N_OUT+1])\n",
    "    true = pd.Series(true, index=test_dates[N_PAST:-N_OUT+1])\n",
    "else:\n",
    "    predictions = pd.Series(predictions, index=test_dates[N_PAST:])\n",
    "    true = pd.Series(true, index=test_dates[N_PAST:])\n",
    "processing.plot_series_predictions(true, predictions, N_OUT=N_OUT, method='LSTM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
