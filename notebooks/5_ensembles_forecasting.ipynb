{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b208a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from solver import processing\n",
    "from solver import ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db6088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FEATURE PARAMETERS\n",
    "# prediction target\n",
    "feature_predict = 'P_avg'\n",
    "# prediction inputs from turbine data\n",
    "features_train = ['P_avg', 'Ba_avg', 'Wa_avg', 'Ya_avg']\n",
    "# engineered prediction inputs \n",
    "engineered_features = ['Wx', 'Wy', 'Day sin', 'Day cos', 'Year sin', 'Year cos']\n",
    "engineered_features = []\n",
    "\n",
    "### TRAIN/VAL/TEST SPLIT\n",
    "train_years = [2013, 2014, 2015, 2016]\n",
    "validation_years = [2017]\n",
    "test_years = [2017]\n",
    "\n",
    "### ENSEMBLE PARAMETERS:\n",
    "N_DT = 500  # (int) number of decision trees\n",
    "N_SVR = 64  # (int) number of svr\n",
    "S_DT = 500  # (int) number of samples for training each decision tree\n",
    "S_SVR = 500 # (int) number of samples for training each svr\n",
    "\n",
    "### FEATURE ENGINEERING PARAMETERS\n",
    "MA_CONSTANT = 3 # moving average smoothing parameter\n",
    "N_OUT = 1 # forecast horizon\n",
    "N_PAST = 6 # number of autoregression samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126984bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets\n",
    "TURBINE_ID = 'R80711'\n",
    "DATA_DIR = os.path.join('../datasets/after_imputation', 'turbine_{}.csv'.format(TURBINE_ID))\n",
    "\n",
    "# read datasets\n",
    "dataset = processing.read_dataset(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68998c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define masks for training/validation and testing (will be used later)\n",
    "train_idx = dataset[dataset['Date_time'].dt.year.isin(train_years)].index\n",
    "valid_idx = dataset[dataset['Date_time'].dt.year.isin(validation_years)].index\n",
    "test_idx = dataset[dataset['Date_time'].dt.year.isin(test_years)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbea3664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: \t\t 0\n",
      "Number of rows with nan: \t 0\n"
     ]
    }
   ],
   "source": [
    "# some stats:\n",
    "print(\"Number of duplicates: \\t\\t {}\".format(len(dataset.index[dataset.index.duplicated()].unique())))\n",
    "print(\"Number of rows with nan: \\t {}\".format(np.count_nonzero(dataset.isnull())))\n",
    "\n",
    "# perform smoothing\n",
    "if feature_predict in features_train:\n",
    "    dataset = processing.smooth(dataset, cols_to_smooth=features_train, ma_constant=MA_CONSTANT)\n",
    "else:\n",
    "    dataset = processing.smooth(dataset, cols_to_smooth=features_train+[feature_predict], ma_constant=MA_CONSTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "262fc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dates for plotting\n",
    "test_dates = dataset.loc[dataset['Date_time'].dt.year.isin(test_years), 'Date_time'].values\n",
    "\n",
    "# split to training/validation/testing sets based on indices\n",
    "dataset_train = dataset[dataset.index.isin(train_idx)].copy()\n",
    "dataset_valid = dataset[dataset.index.isin(valid_idx)].copy()\n",
    "dataset_test = dataset[dataset.index.isin(test_idx)].copy()\n",
    "\n",
    "# define target mask for features\n",
    "target_idx = np.where(dataset_train.columns == feature_predict)[0][0]\n",
    "target_mask = np.zeros((dataset_train.shape[1])).astype(bool)\n",
    "target_mask[target_idx] = True\n",
    "# define input mask for features\n",
    "input_idx = [np.where(dataset_train.columns == feat_col)[0][0] for feat_col in features_train+engineered_features]\n",
    "input_mask = np.zeros((dataset_train.shape[1])).astype(bool)\n",
    "input_mask[input_idx] = True\n",
    "\n",
    "# Define scaler and fit only on training data\n",
    "scaler_output = MinMaxScaler()\n",
    "y_train = scaler_output.fit_transform(dataset_train.iloc[:, target_mask])\n",
    "y_valid = scaler_output.transform(dataset_valid.iloc[:, target_mask])\n",
    "y_test = scaler_output.transform(dataset_test.iloc[:, target_mask])\n",
    "# Define scaler and fit only on training data\n",
    "scaler_input = MinMaxScaler()\n",
    "X_train = scaler_input.fit_transform(dataset_train.iloc[:, input_mask])\n",
    "X_valid = scaler_input.transform(dataset_valid.iloc[:, input_mask])\n",
    "X_test = scaler_input.transform(dataset_test.iloc[:, input_mask])\n",
    "\n",
    "# Make small tests\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_valid.shape[0] == y_valid.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbde5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make supervised learning problem\n",
    "X_train_sup = processing.series_to_supervised(X_train, n_in=N_PAST, n_out=0, dropnan=True)\n",
    "y_train_sup = processing.series_to_supervised(y_train, n_in=N_PAST, n_out=N_OUT, dropnan=True).iloc[:,-1]\n",
    "X_valid_sup = processing.series_to_supervised(X_valid, n_in=N_PAST, n_out=0, dropnan=True)\n",
    "y_valid_sup = processing.series_to_supervised(y_valid, n_in=N_PAST, n_out=N_OUT, dropnan=True).iloc[:,-1]\n",
    "X_test_sup = processing.series_to_supervised(X_test, n_in=N_PAST, n_out=0, dropnan=True)\n",
    "y_test_sup = processing.series_to_supervised(y_test, n_in=N_PAST, n_out=N_OUT, dropnan=True).iloc[:,-1]\n",
    "\n",
    "# Align X with y\n",
    "X_train_sup = X_train_sup[X_train_sup.index.isin(y_train_sup.index)]\n",
    "X_valid_sup = X_valid_sup[X_valid_sup.index.isin(y_valid_sup.index)]\n",
    "X_test_sup = X_test_sup[X_test_sup.index.isin(y_test_sup.index)]\n",
    "\n",
    "# Set to numpy arrays\n",
    "X_train = X_train_sup.values\n",
    "y_train = y_train_sup.values\n",
    "X_valid = X_valid_sup.values\n",
    "y_valid = y_valid_sup.values\n",
    "X_test = X_test_sup.values\n",
    "y_test = y_test_sup.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "718b2c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [03:49,  2.18it/s]\n",
      "  0%|          | 0/8754 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0.14573576].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-647ceaa91045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbase_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreg_estimators1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensembles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_homogeneous_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_DT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_DT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensembles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_estimators1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_OUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Magistras/second_year/wind_power_forecasting/solver/ensembles.py\u001b[0m in \u001b[0;36mmake_predictions\u001b[0;34m(reg_estimators, weights, scaler, X, y, fh)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions_for_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# invert scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# store forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wind_power37/lib/python3.7/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[0;32m--> 459\u001b[0;31m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wind_power37/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/wind_power37/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    639\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# make sure we actually converted to numeric:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0.14573576].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "#  Train FIRST ensemble\n",
    "base_reg = DecisionTreeRegressor(min_samples_split=2, max_depth=50)\n",
    "reg_estimators1, weights1 = ensembles.train_homogeneous_ensemble(base_reg, X_train, y_train, S_DT, N_DT)\n",
    "predictions = ensembles.make_predictions(reg_estimators1, weights1, scaler_output, X_test, y_test, N_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d018db1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
