{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "consistent-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from solver import processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elder-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FEATURE PARAMETERS\n",
    "# prediction target\n",
    "feature_predict = 'P_avg'\n",
    "# prediction inputs from turbine data\n",
    "features_train = ['P_avg', 'Ba_avg', 'Wa_avg', 'Ya_avg']\n",
    "# engineered prediction inputs \n",
    "engineered_features = ['Wx', 'Wy', 'Day sin', 'Day cos', 'Year sin', 'Year cos']\n",
    "\n",
    "### TRAIN/VAL/TEST SPLIT\n",
    "train_years = [2013, 2014, 2015, 2016]\n",
    "validation_years = [2017]\n",
    "test_years = [2017]\n",
    "\n",
    "### TRAINING PARAMETERS:\n",
    "BATCH_SIZE = 512\n",
    "N_EPOCHS = 100\n",
    "N_NEURONS = 64\n",
    "\n",
    "### FEATURE ENGINEERING PARAMETERS\n",
    "MA_CONSTANT = 3 # moving average smoothing parameter\n",
    "N_OUT = 3\n",
    "N_PAST = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "packed-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets\n",
    "TURBINE_ID = 'R80711'\n",
    "DATA_DIR = os.path.join('../datasets/after_imputation', 'turbine_{}.csv'.format(TURBINE_ID))\n",
    "\n",
    "# read datasets\n",
    "dataset = processing.read_dataset(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e8aeb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define masks for training/validation and testing (will be used later)\n",
    "train_idx = dataset[dataset['Date_time'].dt.year.isin(train_years)].index\n",
    "valid_idx = dataset[dataset['Date_time'].dt.year.isin(validation_years)].index\n",
    "test_idx = dataset[dataset['Date_time'].dt.year.isin(test_years)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af0c121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: \t\t 0\n",
      "Number of rows with nan: \t 0\n"
     ]
    }
   ],
   "source": [
    "# some stats:\n",
    "print(\"Number of duplicates: \\t\\t {}\".format(len(dataset.index[dataset.index.duplicated()].unique())))\n",
    "print(\"Number of rows with nan: \\t {}\".format(np.count_nonzero(dataset.isnull())))\n",
    "\n",
    "# perform smoothing\n",
    "if feature_predict in features_train:\n",
    "    dataset = processing.smooth(dataset, cols_to_smooth=features_train, ma_constant=MA_CONSTANT)\n",
    "else:\n",
    "    dataset = processing.smooth(dataset, cols_to_smooth=features_train+[feature_predict], ma_constant=MA_CONSTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65a39262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date_time', 'P_avg', 'Ws_avg', 'Ot_avg', 'Wa_avg', 'Ya_avg', 'Ba_avg',\n",
       "       'Day sin', 'Day cos', 'Year sin', 'Year cos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b62a7994",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0ce0d283e113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# define input mask for features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0minput_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfeat_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_train\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mengineered_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0minput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-0ce0d283e113>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# define input mask for features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0minput_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfeat_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_train\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mengineered_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0minput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# split to training/validation/testing sets based on indices\n",
    "dataset_train = dataset[dataset.index.isin(train_idx)].copy()\n",
    "dataset_valid = dataset[dataset.index.isin(valid_idx)].copy()\n",
    "dataset_test = dataset[dataset.index.isin(test_idx)].copy()\n",
    "\n",
    "# define target mask for features\n",
    "target_idx = np.where(dataset_train.columns == feature_predict)[0][0]\n",
    "target_mask = np.zeros((dataset_train.shape[1])).astype(bool)\n",
    "target_mask[target_idx] = True\n",
    "# define input mask for features\n",
    "input_idx = [np.where(dataset_train.columns == feat_col)[0][0] for feat_col in features_train+engineered_features]\n",
    "input_mask = np.zeros((dataset_train.shape[1])).astype(bool)\n",
    "input_mask[input_idx] = True\n",
    "\n",
    "# Define scaler and fit only on training data\n",
    "scaler_output = MinMaxScaler()\n",
    "y_train = scaler_output.fit_transform(dataset_train.iloc[:, target_mask])\n",
    "y_valid = scaler_output.transform(dataset_valid.iloc[:, target_mask])\n",
    "y_test = scaler_output.transform(dataset_test.iloc[:, target_mask])\n",
    "# Define scaler and fit only on training data\n",
    "scaler_input = MinMaxScaler()\n",
    "X_train = scaler_input.fit_transform(dataset_train.iloc[:, input_mask])\n",
    "X_valid = scaler_input.transform(dataset_valid.iloc[:, input_mask])\n",
    "X_test = scaler_input.transform(dataset_test.iloc[:, input_mask])\n",
    "\n",
    "# Make small tests\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_valid.shape[0] == y_valid.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make supervised learning problem\n",
    "X_train_sup = processing.series_to_supervised(X_train, n_in=N_PAST, n_out=0, dropnan=True)\n",
    "y_train_sup = processing.series_to_supervised(y_train, n_in=N_PAST, n_out=N_OUT, dropnan=True).iloc[:,-1]\n",
    "X_valid_sup = processing.series_to_supervised(X_valid, n_in=N_PAST, n_out=0, dropnan=True)\n",
    "y_valid_sup = processing.series_to_supervised(y_valid, n_in=N_PAST, n_out=N_OUT, dropnan=True).iloc[:,-1]\n",
    "X_test_sup = processing.series_to_supervised(X_test, n_in=N_PAST, n_out=0, dropnan=True)\n",
    "y_test_sup = processing.series_to_supervised(y_test, n_in=N_PAST, n_out=N_OUT, dropnan=True).iloc[:,-1]\n",
    "\n",
    "# Align X with y\n",
    "X_train_sup = X_train_sup[X_train_sup.index.isin(y_train_sup.index)]\n",
    "X_valid_sup = X_valid_sup[X_valid_sup.index.isin(y_valid_sup.index)]\n",
    "X_test_sup = X_test_sup[X_test_sup.index.isin(y_test_sup.index)]\n",
    "\n",
    "# Set to numpy arrays\n",
    "X_train = X_train_sup.values\n",
    "y_train = y_train_sup.values\n",
    "X_valid = X_valid_sup.values\n",
    "y_valid = y_valid_sup.values\n",
    "X_test = X_test_sup.values\n",
    "y_test = y_test_sup.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31adc5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE LSTM MODEL HYPER-PARAMETERS\n",
    "BATCH_SIZE = 512\n",
    "N_EPOCHS = 100\n",
    "N_NEURONS = 64\n",
    "\n",
    "# FIT MODEL\n",
    "lstm_model = fit_lstm(\n",
    "    X_train, y_train, \n",
    "    X_valid, y_valid, \n",
    "    BATCH_SIZE, N_EPOCHS, N_NEURONS, \n",
    "    n_past=N_PAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce068e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions\n",
    "y_predict, y_true = make_predictions(lstm_model, X_valid, y_valid, scaler_output, N_PAST)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
